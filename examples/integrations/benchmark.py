#!/usr/bin/env python3
"""
Memgraph OS Performance Benchmark

This script benchmarks Memgraph OS performance and compares it with:
- Baseline (no memory, full context)
- Memgraph OS (memory-optimized context)

Metrics Measured:
1. Response Time - How fast can we generate responses?
2. Token Usage - How many tokens saved vs full context?
3. Context Assembly - How fast can we retrieve relevant context?
4. Memory Operations - Throughput for add/search operations
5. Accuracy - How well does memory-based context perform?

Based on Mem0's benchmark methodology:
- Mem0 claims: 26% better than OpenAI native memory (LOCOMO benchmark)
- Mem0 claims: 91% faster responses than full-context
- Mem0 claims: 90% lower token usage

Usage:
    export MEMGRAPH_API_KEY=vel_your_key
    export MEMGRAPH_TENANT_ID=your-tenant-id
    export OPENAI_API_KEY=sk-your-key

    python benchmark.py --iterations 100 --output results.json
"""

import os
import sys
import time
import json
import statistics
from typing import List, Dict, Any, Tuple
from datetime import datetime
from dataclasses import dataclass, asdict
import argparse

# Dependencies
try:
    from openai import OpenAI
    import tiktoken
except ImportError:
    print("Missing dependencies. Install with: pip install openai tiktoken")
    sys.exit(1)

# Memgraph SDK
try:
    from memgraph_sdk import MemgraphClient
except ImportError:
    print("Memgraph SDK not found. Install with: pip install memgraph-sdk")
    sys.exit(1)


# ============================================================================
# Data Classes
# ============================================================================

@dataclass
class BenchmarkResult:
    """Results from a single benchmark run"""
    test_name: str
    response_time_ms: float
    token_count: int
    context_assembly_ms: float
    success: bool
    error: str = ""


@dataclass
class BenchmarkSummary:
    """Aggregated benchmark results"""
    test_name: str
    iterations: int
    avg_response_time_ms: float
    median_response_time_ms: float
    p95_response_time_ms: float
    avg_token_count: int
    avg_context_assembly_ms: float
    success_rate: float
    total_duration_s: float


# ============================================================================
# Benchmark Configuration
# ============================================================================

# Test conversation scenarios
TEST_SCENARIOS = [
    {
        "setup": [
            "My name is Alex and I'm a software engineer at TechCorp",
            "I prefer Python for backend and React for frontend",
            "We use PostgreSQL as our primary database",
            "I'm working on a microservices architecture project",
            "The project uses Docker and Kubernetes for deployment"
        ],
        "queries": [
            "What's my name and role?",
            "What programming languages do I prefer?",
            "What database technology are we using?",
            "What's the architecture of my current project?",
            "How do we deploy our services?"
        ]
    },
    {
        "setup": [
            "User reported bug: Authentication fails after 24 hours",
            "Investigation: JWT tokens expire after 1 day",
            "Solution: Increased token TTL to 7 days",
            "Deploy: Rolled out fix to production",
            "Result: No more authentication timeout complaints"
        ],
        "queries": [
            "What was the authentication bug?",
            "How did we fix the JWT issue?",
            "What was changed in production?",
            "What was the result of our fix?",
            "How long do tokens last now?"
        ]
    },
    {
        "setup": [
            "Team decision: Migrate from REST to GraphQL",
            "Rationale: Reduce over-fetching and under-fetching",
            "Timeline: 6 weeks for complete migration",
            "Phase 1: New endpoints in GraphQL",
            "Phase 2: Deprecate old REST endpoints"
        ],
        "queries": [
            "What API technology are we migrating to?",
            "Why are we making this change?",
            "How long will the migration take?",
            "What's the first phase of migration?",
            "What happens to REST endpoints?"
        ]
    }
]


# ============================================================================
# Benchmark Runner
# ============================================================================

class MemgraphBenchmark:
    """Performance benchmark for Memgraph OS"""

    def __init__(
        self,
        openai_api_key: str,
        memgraph_api_key: str,
        memgraph_tenant_id: str,
        memgraph_base_url: str = None
    ):
        """
        Initialize benchmark.

        Args:
            openai_api_key: OpenAI API key
            memgraph_api_key: Memgraph API key
            memgraph_tenant_id: Memgraph tenant ID
            memgraph_base_url: Memgraph API URL
        """
        memgraph_base_url = memgraph_base_url or os.getenv("MEMGRAPH_API_URL", "http://localhost:8001/v1")
        self.openai = OpenAI(api_key=openai_api_key)
        self.memgraph = MemgraphClient(
            api_key=memgraph_api_key,
            tenant_id=memgraph_tenant_id,
            base_url=memgraph_base_url
        )
        self.encoding = tiktoken.encoding_for_model("gpt-4")

    def count_tokens(self, text: str) -> int:
        """Count tokens in text."""
        return len(self.encoding.encode(text))

    def run_baseline_test(
        self,
        scenario: Dict[str, List[str]],
        query: str,
        user_id: str
    ) -> BenchmarkResult:
        """
        Run baseline test (full context, no memory).

        Args:
            scenario: Test scenario with setup messages
            query: Query to test
            user_id: User identifier

        Returns:
            BenchmarkResult
        """
        start_time = time.time()

        try:
            # Build full context prompt (all history)
            context_text = "\n".join([
                f"Context {i+1}: {msg}"
                for i, msg in enumerate(scenario["setup"])
            ])

            system_prompt = f"""You are a helpful assistant.

Here is the full conversation history:
{context_text}

Answer the user's question based on this history."""

            # Generate response
            llm_start = time.time()
            response = self.openai.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": query}
                ],
                temperature=0.3,
                max_tokens=200
            )
            llm_end = time.time()

            # Calculate metrics
            response_time_ms = (llm_end - llm_start) * 1000
            token_count = self.count_tokens(system_prompt) + self.count_tokens(query)

            return BenchmarkResult(
                test_name="baseline_full_context",
                response_time_ms=response_time_ms,
                token_count=token_count,
                context_assembly_ms=0,  # No context assembly
                success=True
            )

        except Exception as e:
            return BenchmarkResult(
                test_name="baseline_full_context",
                response_time_ms=(time.time() - start_time) * 1000,
                token_count=0,
                context_assembly_ms=0,
                success=False,
                error=str(e)
            )

    def run_memgraph_test(
        self,
        scenario: Dict[str, List[str]],
        query: str,
        user_id: str
    ) -> BenchmarkResult:
        """
        Run Memgraph test (memory-optimized context).

        Args:
            scenario: Test scenario with setup messages
            query: Query to test
            user_id: User identifier

        Returns:
            BenchmarkResult
        """
        start_time = time.time()

        try:
            # Context assembly
            context_start = time.time()
            context = self.memgraph.search(query=query, user_id=user_id)
            context_end = time.time()
            context_assembly_ms = (context_end - context_start) * 1000

            # Build optimized prompt (only relevant context)
            beliefs = context.get("beliefs", [])[:3]
            memories = context.get("memories", [])[:2]

            context_text = ""
            if beliefs:
                context_text += "Relevant facts:\n"
                for belief in beliefs:
                    if isinstance(belief, dict):
                        context_text += f"- {belief.get('key')}: {belief.get('value')}\n"

            if memories:
                context_text += "\nRelevant context:\n"
                for memory in memories:
                    if isinstance(memory, dict):
                        text = memory.get("text", memory.get("content", ""))
                        if isinstance(text, dict):
                            text = text.get("text", str(text))
                        context_text += f"- {text}\n"

            system_prompt = f"""You are a helpful assistant with memory.

{context_text}

Answer the user's question based on the relevant context above."""

            # Generate response
            llm_start = time.time()
            response = self.openai.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": query}
                ],
                temperature=0.3,
                max_tokens=200
            )
            llm_end = time.time()

            # Calculate metrics
            response_time_ms = (llm_end - llm_start) * 1000
            token_count = self.count_tokens(system_prompt) + self.count_tokens(query)

            return BenchmarkResult(
                test_name="memgraph_memory_optimized",
                response_time_ms=response_time_ms,
                token_count=token_count,
                context_assembly_ms=context_assembly_ms,
                success=True
            )

        except Exception as e:
            return BenchmarkResult(
                test_name="memgraph_memory_optimized",
                response_time_ms=(time.time() - start_time) * 1000,
                token_count=0,
                context_assembly_ms=0,
                success=False,
                error=str(e)
            )

    def setup_scenario(self, scenario: Dict[str, List[str]], user_id: str):
        """Store scenario setup messages in Memgraph."""
        print(f"üìù Setting up scenario for user {user_id}...")
        for message in scenario["setup"]:
            self.memgraph.add(text=message, user_id=user_id)
            time.sleep(0.1)  # Avoid rate limits

        # Wait for processing
        print("‚è≥ Waiting 2s for memory consolidation...")
        time.sleep(2)

    def run_benchmark(
        self,
        iterations: int = 10,
        scenarios: List[Dict] = None
    ) -> Tuple[List[BenchmarkSummary], Dict[str, Any]]:
        """
        Run complete benchmark suite.

        Args:
            iterations: Number of iterations per test
            scenarios: Test scenarios (uses defaults if None)

        Returns:
            Tuple of (summaries, comparison_stats)
        """
        scenarios = scenarios or TEST_SCENARIOS
        baseline_results = []
        memgraph_results = []

        print("\nüèÅ Starting Memgraph Performance Benchmark")
        print("=" * 60)
        print(f"Iterations per test: {iterations}")
        print(f"Scenarios: {len(scenarios)}")
        print(f"Total tests: {len(scenarios) * iterations * 2}")
        print("=" * 60)

        for scenario_idx, scenario in enumerate(scenarios):
            print(f"\nüìä Scenario {scenario_idx + 1}/{len(scenarios)}")

            # Setup scenario
            user_id = f"benchmark_user_{scenario_idx}_{int(time.time())}"
            self.setup_scenario(scenario, user_id)

            # Run tests
            for iteration in range(iterations):
                query = scenario["queries"][iteration % len(scenario["queries"])]

                print(f"  Iteration {iteration + 1}/{iterations}: {query[:50]}...")

                # Baseline test
                baseline_result = self.run_baseline_test(scenario, query, user_id)
                baseline_results.append(baseline_result)

                # Memgraph test
                memgraph_result = self.run_memgraph_test(scenario, query, user_id)
                memgraph_results.append(memgraph_result)

                time.sleep(0.5)  # Rate limiting

        # Calculate summaries
        baseline_summary = self._calculate_summary("Baseline (Full Context)", baseline_results)
        memgraph_summary = self._calculate_summary("Memgraph (Memory-Optimized)", memgraph_results)

        # Calculate comparison stats
        comparison = self._calculate_comparison(baseline_summary, memgraph_summary)

        return [baseline_summary, memgraph_summary], comparison

    def _calculate_summary(self, test_name: str, results: List[BenchmarkResult]) -> BenchmarkSummary:
        """Calculate summary statistics from results."""
        successful = [r for r in results if r.success]

        if not successful:
            return BenchmarkSummary(
                test_name=test_name,
                iterations=len(results),
                avg_response_time_ms=0,
                median_response_time_ms=0,
                p95_response_time_ms=0,
                avg_token_count=0,
                avg_context_assembly_ms=0,
                success_rate=0,
                total_duration_s=0
            )

        response_times = [r.response_time_ms for r in successful]
        token_counts = [r.token_count for r in successful]
        context_times = [r.context_assembly_ms for r in successful]

        return BenchmarkSummary(
            test_name=test_name,
            iterations=len(results),
            avg_response_time_ms=statistics.mean(response_times),
            median_response_time_ms=statistics.median(response_times),
            p95_response_time_ms=statistics.quantiles(response_times, n=20)[18],
            avg_token_count=int(statistics.mean(token_counts)),
            avg_context_assembly_ms=statistics.mean(context_times),
            success_rate=len(successful) / len(results),
            total_duration_s=sum(response_times) / 1000
        )

    def _calculate_comparison(
        self,
        baseline: BenchmarkSummary,
        memgraph: BenchmarkSummary
    ) -> Dict[str, Any]:
        """Calculate improvement percentages."""
        response_time_improvement = (
            (baseline.avg_response_time_ms - memgraph.avg_response_time_ms)
            / baseline.avg_response_time_ms * 100
        )

        token_reduction = (
            (baseline.avg_token_count - memgraph.avg_token_count)
            / baseline.avg_token_count * 100
        )

        return {
            "response_time_improvement_pct": round(response_time_improvement, 2),
            "token_reduction_pct": round(token_reduction, 2),
            "context_assembly_overhead_ms": round(memgraph.avg_context_assembly_ms, 2),
            "baseline_avg_ms": round(baseline.avg_response_time_ms, 2),
            "memgraph_avg_ms": round(memgraph.avg_response_time_ms, 2),
            "baseline_avg_tokens": baseline.avg_token_count,
            "memgraph_avg_tokens": memgraph.avg_token_count
        }


# ============================================================================
# CLI and Output
# ============================================================================

def print_results(summaries: List[BenchmarkSummary], comparison: Dict[str, Any]):
    """Print benchmark results to console."""
    print("\n" + "=" * 60)
    print("üìä BENCHMARK RESULTS")
    print("=" * 60)

    for summary in summaries:
        print(f"\n{summary.test_name}:")
        print(f"  Avg Response Time:    {summary.avg_response_time_ms:.2f} ms")
        print(f"  Median Response Time: {summary.median_response_time_ms:.2f} ms")
        print(f"  P95 Response Time:    {summary.p95_response_time_ms:.2f} ms")
        print(f"  Avg Token Count:      {summary.avg_token_count:,}")
        print(f"  Context Assembly:     {summary.avg_context_assembly_ms:.2f} ms")
        print(f"  Success Rate:         {summary.success_rate:.1%}")

    print("\n" + "=" * 60)
    print("üéØ COMPARISON")
    print("=" * 60)
    print(f"Response Time Improvement: {comparison['response_time_improvement_pct']:+.1f}%")
    print(f"Token Reduction:           {comparison['token_reduction_pct']:+.1f}%")
    print(f"Context Assembly Overhead: {comparison['context_assembly_overhead_ms']:.2f} ms")

    print("\nüí° Interpretation:")
    if comparison['response_time_improvement_pct'] > 0:
        print(f"  ‚úÖ Memgraph is {abs(comparison['response_time_improvement_pct']):.1f}% faster")
    else:
        print(f"  ‚ö†Ô∏è  Baseline is {abs(comparison['response_time_improvement_pct']):.1f}% faster")

    if comparison['token_reduction_pct'] > 0:
        print(f"  ‚úÖ Memgraph uses {abs(comparison['token_reduction_pct']):.1f}% fewer tokens")
    else:
        print(f"  ‚ö†Ô∏è  Baseline uses {abs(comparison['token_reduction_pct']):.1f}% fewer tokens")


def save_results(summaries: List[BenchmarkSummary], comparison: Dict[str, Any], output_file: str):
    """Save results to JSON file."""
    data = {
        "timestamp": datetime.utcnow().isoformat(),
        "summaries": [asdict(s) for s in summaries],
        "comparison": comparison
    }

    with open(output_file, 'w') as f:
        json.dump(data, f, indent=2)

    print(f"\nüíæ Results saved to: {output_file}")


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Memgraph OS Performance Benchmark")
    parser.add_argument("--iterations", type=int, default=10, help="Iterations per test")
    parser.add_argument("--output", type=str, default="benchmark_results.json", help="Output file")
    args = parser.parse_args()

    # Check environment
    if not all([
        os.getenv("OPENAI_API_KEY"),
        os.getenv("MEMGRAPH_API_KEY"),
        os.getenv("MEMGRAPH_TENANT_ID")
    ]):
        print("‚ùå Missing environment variables!")
        print("\nRequired:")
        print("  export OPENAI_API_KEY=sk-your-key")
        print("  export MEMGRAPH_API_KEY=vel_your_key")
        print("  export MEMGRAPH_TENANT_ID=your-tenant-id")
        sys.exit(1)

    # Run benchmark
    benchmark = MemgraphBenchmark(
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        memgraph_api_key=os.getenv("MEMGRAPH_API_KEY"),
        memgraph_tenant_id=os.getenv("MEMGRAPH_TENANT_ID")
    )

    summaries, comparison = benchmark.run_benchmark(iterations=args.iterations)

    # Output results
    print_results(summaries, comparison)
    save_results(summaries, comparison, args.output)

    print("\n‚úÖ Benchmark complete!")


if __name__ == "__main__":
    main()
